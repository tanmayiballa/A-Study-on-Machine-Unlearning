{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:07.215301Z",
     "iopub.status.busy": "2023-12-05T02:33:07.215007Z",
     "iopub.status.idle": "2023-12-05T02:33:09.367767Z",
     "shell.execute_reply": "2023-12-05T02:33:09.366976Z",
     "shell.execute_reply.started": "2023-12-05T02:33:07.215270Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: CUDA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, model_selection\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import resnet18\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "from sklearn import linear_model, model_selection\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "# manual random seed is used for dataset partitioning\n",
    "# to ensure reproducible results across runs\n",
    "RNG = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the dataset and creating the train_loader, retain_loader, test_loader, forget_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:09.369233Z",
     "iopub.status.busy": "2023-12-05T02:33:09.368857Z",
     "iopub.status.idle": "2023-12-05T02:33:10.975729Z",
     "shell.execute_reply": "2023-12-05T02:33:10.974992Z",
     "shell.execute_reply.started": "2023-12-05T02:33:09.369206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# download and pre-process CIFAR10\n",
    "normalize = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=normalize\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# we split held out data into test and validation set\n",
    "held_out = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=normalize\n",
    ")\n",
    "test_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# download the forget and retain index split\n",
    "local_path = \"forget_idx.npy\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://storage.googleapis.com/unlearning-challenge/\" + local_path\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "forget_idx = np.load(local_path)\n",
    "\n",
    "# construct indices of retain from those of the forget set\n",
    "forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
    "forget_mask[forget_idx] = True\n",
    "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
    "\n",
    "# split train set into a forget and a retain set\n",
    "forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
    "retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(\n",
    "    forget_set, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "retain_loader = torch.utils.data.DataLoader(\n",
    "    retain_set, batch_size=128, shuffle=True, num_workers=2, generator=RNG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:10.977301Z",
     "iopub.status.busy": "2023-12-05T02:33:10.977027Z",
     "iopub.status.idle": "2023-12-05T02:33:12.575837Z",
     "shell.execute_reply": "2023-12-05T02:33:12.574892Z",
     "shell.execute_reply.started": "2023-12-05T02:33:10.977276Z"
    }
   },
   "outputs": [],
   "source": [
    "# download pre-trained weights\n",
    "local_path = \"weights_resnet18_cifar10.pth\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://storage.googleapis.com/unlearning-challenge/weights_resnet18_cifar10.pth\"\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "\n",
    "weights_pretrained = torch.load(local_path, map_location=DEVICE)\n",
    "\n",
    "# load model with pre-trained weights\n",
    "model = resnet18(weights=None, num_classes=10)\n",
    "model.load_state_dict(weights_pretrained)\n",
    "model.to(DEVICE)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:12.579713Z",
     "iopub.status.busy": "2023-12-05T02:33:12.578997Z",
     "iopub.status.idle": "2023-12-05T02:33:22.585072Z",
     "shell.execute_reply": "2023-12-05T02:33:22.583931Z",
     "shell.execute_reply.started": "2023-12-05T02:33:12.579652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 99.5%\n",
      "Test set accuracy: 88.3%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(net, loader):\n",
    "    \"\"\"Return accuracy on a dataset given by the data loader.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "print(f\"Train set accuracy: {100.0 * accuracy(model, train_loader):0.1f}%\")\n",
    "print(f\"Test set accuracy: {100.0 * accuracy(model, test_loader):0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a retrain model to compare with our unlearning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:22.587225Z",
     "iopub.status.busy": "2023-12-05T02:33:22.586834Z",
     "iopub.status.idle": "2023-12-05T02:33:30.869725Z",
     "shell.execute_reply": "2023-12-05T02:33:30.868571Z",
     "shell.execute_reply.started": "2023-12-05T02:33:22.587190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain set accuracy: 99.5%\n",
      "Forget set accuracy: 88.2%\n"
     ]
    }
   ],
   "source": [
    "# download weights of a model trained exclusively on the retain set\n",
    "local_path = \"retrain_weights_resnet18_cifar10.pth\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://storage.googleapis.com/unlearning-challenge/\" + local_path\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "\n",
    "weights_pretrained = torch.load(local_path, map_location=DEVICE)\n",
    "\n",
    "# load model with pre-trained weights\n",
    "rt_model = resnet18(weights=None, num_classes=10)\n",
    "rt_model.load_state_dict(weights_pretrained)\n",
    "rt_model.to(DEVICE)\n",
    "rt_model.eval()\n",
    "\n",
    "# print its accuracy on retain and forget set\n",
    "print(f\"Retain set accuracy: {100.0 * accuracy(rt_model, retain_loader):0.1f}%\")\n",
    "print(f\"Forget set accuracy: {100.0 * accuracy(rt_model, forget_loader):0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.873057Z",
     "iopub.status.busy": "2023-12-05T02:33:30.872711Z",
     "iopub.status.idle": "2023-12-05T02:33:30.881610Z",
     "shell.execute_reply": "2023-12-05T02:33:30.880738Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.873029Z"
    }
   },
   "outputs": [],
   "source": [
    "def unlearning(net, retain, forget, validation):\n",
    "    \"\"\"Unlearning by fine-tuning.\n",
    "\n",
    "    Fine-tuning is a very simple algorithm that trains using only\n",
    "    the retain set.\n",
    "\n",
    "    Args:\n",
    "      net : nn.Module.\n",
    "        pre-trained model to use as base of unlearning.\n",
    "      retain : torch.utils.data.DataLoader.\n",
    "        Dataset loader for access to the retain set. This is the subset\n",
    "        of the training set that we don't want to forget.\n",
    "      forget : torch.utils.data.DataLoader.\n",
    "        Dataset loader for access to the forget set. This is the subset\n",
    "        of the training set that we want to forget. This method doesn't\n",
    "        make use of the forget set.\n",
    "      validation : torch.utils.data.DataLoader.\n",
    "        Dataset loader for access to the validation set. This method doesn't\n",
    "        make use of the validation set.\n",
    "    Returns:\n",
    "      net : updated model\n",
    "    \"\"\"\n",
    "    epochs = 5\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    net.train()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for inputs, targets in retain:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    net.eval()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.883060Z",
     "iopub.status.busy": "2023-12-05T02:33:30.882797Z",
     "iopub.status.idle": "2023-12-05T02:33:30.895150Z",
     "shell.execute_reply": "2023-12-05T02:33:30.894316Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.883036Z"
    }
   },
   "outputs": [],
   "source": [
    "data_loaders = {\n",
    "    \"retain\" : retain_loader,\n",
    "    \"forget\" : forget_loader,\n",
    "    \"validation\" : val_loader,\n",
    "    \"testing\" : test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.896874Z",
     "iopub.status.busy": "2023-12-05T02:33:30.896251Z",
     "iopub.status.idle": "2023-12-05T02:33:30.904216Z",
     "shell.execute_reply": "2023-12-05T02:33:30.903346Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.896839Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_models = {\n",
    "    \"original\" : model,\n",
    "    \"retrained\" : rt_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.905596Z",
     "iopub.status.busy": "2023-12-05T02:33:30.905345Z",
     "iopub.status.idle": "2023-12-05T02:33:30.913213Z",
     "shell.execute_reply": "2023-12-05T02:33:30.912280Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.905567Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_outputs(net, loader):\n",
    "    \"\"\"Auxiliary function to compute the logits for all datapoints.\n",
    "    Does not shuffle the data, regardless of the loader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure loader does not shuffle the data\n",
    "    if isinstance(loader.sampler, torch.utils.data.sampler.RandomSampler):\n",
    "        loader = DataLoader(\n",
    "            loader.dataset, \n",
    "            batch_size=loader.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=loader.num_workers)\n",
    "    \n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(\"cpu\")\n",
    "\n",
    "        logits = net(inputs).detach().cpu().numpy() # (batch_size, num_classes)\n",
    "        \n",
    "        all_outputs.append(logits)\n",
    "        all_targets.extend(targets)\n",
    "        \n",
    "    return np.array(all_targets), np.concatenate(all_outputs) # (len(loader.dataset), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.914562Z",
     "iopub.status.busy": "2023-12-05T02:33:30.914266Z",
     "iopub.status.idle": "2023-12-05T02:33:30.925173Z",
     "shell.execute_reply": "2023-12-05T02:33:30.924361Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.914537Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_f(x, targets):\n",
    "    # Convert NumPy array to PyTorch tensor\n",
    "    x_tensor = torch.from_numpy(x)\n",
    "\n",
    "    # Apply softmax to the model output\n",
    "    x_softmax = F.softmax(x_tensor, dim=-1)\n",
    "\n",
    "    # Convert targets to one-hot encoding\n",
    "    targets_tensor = torch.from_numpy(targets)\n",
    "    targets_one_hot = F.one_hot(targets_tensor, num_classes=x_tensor.shape[-1])\n",
    "\n",
    "    # Avoiding NaN values in x\n",
    "    x_tensor[torch.isnan(x_tensor)] = 0.0\n",
    "\n",
    "    # Calculate cross-entropy loss for each example\n",
    "    loss = -torch.sum(targets_one_hot * torch.log(x_softmax), dim=-1)\n",
    "\n",
    "    # Convert the result back to a NumPy array if needed\n",
    "    loss_np = loss.numpy()\n",
    "\n",
    "    return loss_np\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.926640Z",
     "iopub.status.busy": "2023-12-05T02:33:30.926373Z",
     "iopub.status.idle": "2023-12-05T02:33:30.937592Z",
     "shell.execute_reply": "2023-12-05T02:33:30.936793Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.926616Z"
    }
   },
   "outputs": [],
   "source": [
    "def false_positive_rate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Computes the false positive rate (FPR).\"\"\"\n",
    "    fp = np.sum(np.logical_and((y_pred == 1), (y_true == 0)))\n",
    "    n = np.sum(y_true == 0)\n",
    "    return fp / n\n",
    "\n",
    "\n",
    "def false_negative_rate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Computes the false negative rate (FNR).\"\"\"\n",
    "    fn = np.sum(np.logical_and((y_pred == 0), (y_true == 1)))\n",
    "    p = np.sum(y_true == 1)\n",
    "    return fn / p\n",
    "\n",
    "\n",
    "# The SCORING dictionary is used by sklearn's `cross_validate` function so that\n",
    "# we record the FPR and FNR metrics of interest when doing cross validation\n",
    "SCORING = {\n",
    "    'false_positive_rate': make_scorer(false_positive_rate),\n",
    "    'false_negative_rate': make_scorer(false_negative_rate)\n",
    "}\n",
    "\n",
    "\n",
    "def logistic_regression_attack(\n",
    "        outputs_U, outputs_R, n_splits=2, random_state=0):\n",
    "    \"\"\"Computes cross-validation score of a membership inference attack.\n",
    "\n",
    "    Args:\n",
    "      outputs_U: numpy array of shape (N)\n",
    "      outputs_R: numpy array of shape (N)\n",
    "      n_splits: int\n",
    "        number of splits to use in the cross-validation.\n",
    "    Returns:\n",
    "      fpr, fnr : float * float\n",
    "    \"\"\"\n",
    "    assert len(outputs_U) == len(outputs_R)\n",
    "    \n",
    "    samples = np.concatenate((outputs_R, outputs_U)).reshape((-1, 1))\n",
    "    labels = np.array([0] * len(outputs_R) + [1] * len(outputs_U))\n",
    "\n",
    "    attack_model = linear_model.LogisticRegression()\n",
    "    cv = model_selection.StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, random_state=random_state\n",
    "    )\n",
    "    scores =  model_selection.cross_validate(\n",
    "        attack_model, samples, labels, cv=cv, scoring=SCORING)\n",
    "    \n",
    "    fpr = np.mean(scores[\"test_false_positive_rate\"])\n",
    "    fnr = np.mean(scores[\"test_false_negative_rate\"])\n",
    "    \n",
    "    return fpr, fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.938921Z",
     "iopub.status.busy": "2023-12-05T02:33:30.938623Z",
     "iopub.status.idle": "2023-12-05T02:33:30.950829Z",
     "shell.execute_reply": "2023-12-05T02:33:30.950100Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.938898Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_epsilon_s(fpr: list[float], fnr: list[float], delta: float) -> float:\n",
    "    \"\"\"Computes the privacy degree (epsilon) of a particular forget set example, \n",
    "    given the FPRs and FNRs resulting from various attacks.\n",
    "    \n",
    "    The smaller epsilon is, the better the unlearning is.\n",
    "    \n",
    "    Args:\n",
    "      fpr: list[float] of length m = num attacks. The FPRs for a particular example. \n",
    "      fnr: list[float] of length m = num attacks. The FNRs for a particular example.\n",
    "      delta: float\n",
    "    Returns:\n",
    "      epsilon: float corresponding to the privacy degree of the particular example.\n",
    "    \"\"\"\n",
    "    assert len(fpr) == len(fnr)\n",
    "    \n",
    "    per_attack_epsilon = [0.]\n",
    "    for fpr_i, fnr_i in zip(fpr, fnr):\n",
    "        if fpr_i == 0 and fnr_i == 0:\n",
    "            per_attack_epsilon.append(np.inf)\n",
    "        elif fpr_i == 0 or fnr_i == 0:\n",
    "            pass # discard attack\n",
    "        else:\n",
    "            with np.errstate(invalid='ignore'):\n",
    "                epsilon1 = np.log(1. - delta - fpr_i) - np.log(fnr_i)\n",
    "                epsilon2 = np.log(1. - delta - fnr_i) - np.log(fpr_i)\n",
    "            if np.isnan(epsilon1) and np.isnan(epsilon2):\n",
    "                per_attack_epsilon.append(np.inf)\n",
    "            else:\n",
    "                per_attack_epsilon.append(np.nanmax([epsilon1, epsilon2]))\n",
    "            \n",
    "    return np.nanmax(per_attack_epsilon)\n",
    "\n",
    "\n",
    "def bin_index_fn(\n",
    "        epsilons: np.ndarray, \n",
    "        bin_width: float = 0.5, \n",
    "        B: int = 13\n",
    "        ) -> np.ndarray:\n",
    "    \"\"\"The bin index function.\"\"\"\n",
    "    bins = np.arange(0, B) * bin_width\n",
    "    return np.digitize(epsilons, bins)\n",
    "\n",
    "\n",
    "def H(epsilons: np.ndarray) -> float:\n",
    "    \"\"\"Computes the forgetting quality given the privacy degrees \n",
    "    of the forget set examples.\n",
    "    \"\"\"\n",
    "    ns = bin_index_fn(epsilons)\n",
    "    hs = 2. / 2 ** ns\n",
    "    return np.mean(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.955073Z",
     "iopub.status.busy": "2023-12-05T02:33:30.954830Z",
     "iopub.status.idle": "2023-12-05T02:33:30.964511Z",
     "shell.execute_reply": "2023-12-05T02:33:30.963635Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.955052Z"
    }
   },
   "outputs": [],
   "source": [
    "def forgetting_quality(\n",
    "        outputs_U: np.ndarray, # (N, S)\n",
    "        outputs_R: np.ndarray, # (N, S)\n",
    "        attacks: list[Callable] = [logistic_regression_attack],\n",
    "        delta: float = 0.01\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Both `outputs_U` and `outputs_R` are of numpy arrays of ndim 2:\n",
    "    * 1st dimension coresponds to the number of samples obtained from the \n",
    "      distribution of each model (N=512 in the case of the competition's leaderboard) \n",
    "    * 2nd dimension corresponds to the number of samples in the forget set (S).\n",
    "    \"\"\"\n",
    "    \n",
    "    # N = number of model samples\n",
    "    # S = number of forget samples\n",
    "    N, S = outputs_U.shape\n",
    "    \n",
    "    assert outputs_U.shape == outputs_R.shape, \\\n",
    "        \"unlearn and retrain outputs need to be of the same shape\"\n",
    "    \n",
    "    epsilons = []\n",
    "    pbar = tqdm(range(S))\n",
    "    for sample_id in pbar:\n",
    "        pbar.set_description(\"Computing F...\")\n",
    "        \n",
    "        sample_fprs, sample_fnrs = [], []\n",
    "        \n",
    "        for attack in attacks: \n",
    "            uls = outputs_U[:, sample_id]\n",
    "            rls = outputs_R[:, sample_id]\n",
    "            \n",
    "            fpr, fnr = attack(uls, rls)\n",
    "            \n",
    "            if isinstance(fpr, list):\n",
    "                sample_fprs.extend(fpr)\n",
    "                sample_fnrs.extend(fnr)\n",
    "            else:\n",
    "                sample_fprs.append(fpr)\n",
    "                sample_fnrs.append(fnr)\n",
    "        \n",
    "        sample_epsilon = compute_epsilon_s(sample_fprs, sample_fnrs, delta=delta)\n",
    "        epsilons.append(sample_epsilon)\n",
    "        \n",
    "    return H(np.array(epsilons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.966244Z",
     "iopub.status.busy": "2023-12-05T02:33:30.965925Z",
     "iopub.status.idle": "2023-12-05T02:33:30.981786Z",
     "shell.execute_reply": "2023-12-05T02:33:30.980889Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.966213Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_unlearning_algorithm(\n",
    "        data_loaders: dict, \n",
    "        pretrained_models: dict, \n",
    "        unlearning: Callable, \n",
    "        n: int = 10,\n",
    "        delta: float = 0.01,\n",
    "        f: Callable = cross_entropy_f,\n",
    "        attacks: list[Callable] = [logistic_regression_attack]\n",
    "        ) -> dict:\n",
    "    \n",
    "    retain_loader = data_loaders[\"retain\"]\n",
    "    forget_loader = data_loaders[\"forget\"]\n",
    "    val_loader = data_loaders[\"validation\"]\n",
    "    test_loader = data_loaders[\"testing\"]\n",
    "    \n",
    "    original_model = pretrained_models[\"original\"]\n",
    "    rt_model = pretrained_models[\"retrained\"]\n",
    "    \n",
    "    outputs_U = []\n",
    "    outputs_R = []\n",
    "    retain_accuracy = []\n",
    "    test_accuracy = []\n",
    "    forget_accuracy = []\n",
    "    \n",
    "    u_model = deepcopy(original_model)\n",
    "    for i in range(n):\n",
    "        print(\"Running epoch :\",i+1)\n",
    "        \n",
    "        print(\"I am now unlearning all the wrong things you taught me!!!\")\n",
    "        \n",
    "        u_model = unlearning(u_model, retain_loader, forget_loader, val_loader)\n",
    "        \n",
    "        targets, outputs_Ui = compute_outputs(u_model, forget_loader)\n",
    "        \n",
    "#         print(targets.shape)\n",
    "#         f_forget = f(outputs_Ui, targets)\n",
    "#         print(f_forget.shape)\n",
    "        \n",
    "        outputs_U.append( f(outputs_Ui, targets) )\n",
    "        \n",
    "        print(\"Computing retain accuracy on Unlearning Model\")\n",
    "        acc = accuracy(u_model, retain_loader)\n",
    "        print(\"Retain accuracy on Unlearning Model is \",acc)\n",
    "        retain_accuracy.append(acc)\n",
    "        \n",
    "        print(\"Computing test accuracy on Unlearning model\")\n",
    "        acc = accuracy(u_model, test_loader)\n",
    "        print(\"Test accuracy on Unlearning model is \", acc)\n",
    "        test_accuracy.append(acc)\n",
    "        \n",
    "        print(\"Computing forget accuracy on Unlearning model\")\n",
    "        acc = accuracy(u_model, forget_loader)\n",
    "        print(\"Forget accuracy on Unlearning model is \",acc)\n",
    "        forget_accuracy.append(acc)\n",
    "        \n",
    "    outputs_U = np.array(outputs_U)\n",
    "    print(\"Printing outputs_U shape \",outputs_U.shape)\n",
    "#     print(outputs_U.shape)\n",
    "    \n",
    "    assert outputs_U.shape == (n, len(forget_loader.dataset)),\\\n",
    "        \"Wrong shape for outputs_U. Should be (num_model_samples, num_forget_datapoints).\"\n",
    "    \n",
    "    RAR = accuracy(rt_model, retain_loader)\n",
    "    TAR = accuracy(rt_model, test_loader)\n",
    "    FAR = accuracy(rt_model, forget_loader)\n",
    "    \n",
    "    RAU = np.mean(retain_accuracy)\n",
    "    TAU = np.mean(test_accuracy)\n",
    "    FAU = np.mean(forget_accuracy)\n",
    "    \n",
    "    RA_ratio = RAU / RAR\n",
    "    TA_ratio = TAU / TAR\n",
    "    \n",
    "    for i in range(n):\n",
    "        targets, outputs_Ri = compute_outputs(rt_model, forget_loader) #(len(forget_loader.dataset), 10) \n",
    "        \n",
    "        outputs_R.append(f(outputs_Ri, targets) )\n",
    "    \n",
    "    outputs_R = np.array(outputs_R)\n",
    "    print(\"Printing outputs_R shape \",outputs_R.shape)\n",
    "    \n",
    "    f = forgetting_quality(\n",
    "    outputs_U, \n",
    "    outputs_R,\n",
    "    attacks=attacks,\n",
    "    delta=delta)\n",
    "    \n",
    "    return {\n",
    "        \"total_score\": f * RA_ratio * TA_ratio,\n",
    "        \"F\": f,\n",
    "        \"unlearn_retain_accuracy\": RAU,\n",
    "        \"unlearn_test_accuracy\": TAU, \n",
    "        \"unlearn_forget_accuracy\": FAU,\n",
    "        \"retrain_retain_accuracy\": RAR,\n",
    "        \"retrain_test_accuracy\": TAR, \n",
    "        \"retrain_forget_accuracy\": FAR,\n",
    "        \"retrain_outputs\": outputs_R,\n",
    "        \"unlearn_outputs\": outputs_U,\n",
    "        \"unlearning_model\": u_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.983214Z",
     "iopub.status.busy": "2023-12-05T02:33:30.982921Z",
     "iopub.status.idle": "2023-12-05T02:33:30.993982Z",
     "shell.execute_reply": "2023-12-05T02:33:30.993118Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.983190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "print(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:30.995295Z",
     "iopub.status.busy": "2023-12-05T02:33:30.994966Z",
     "iopub.status.idle": "2023-12-05T02:33:31.004815Z",
     "shell.execute_reply": "2023-12-05T02:33:31.003928Z",
     "shell.execute_reply.started": "2023-12-05T02:33:30.995263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(forget_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:33:31.006050Z",
     "iopub.status.busy": "2023-12-05T02:33:31.005753Z",
     "iopub.status.idle": "2023-12-05T02:53:26.141545Z",
     "shell.execute_reply": "2023-12-05T02:53:26.140425Z",
     "shell.execute_reply.started": "2023-12-05T02:33:31.006027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch : 1\n",
      "I am now unlearning all the wrong things you taught me!!!\n",
      "Computing retain accuracy on Unlearning Model\n",
      "Retain accuracy on Unlearning Model is  0.9889555555555556\n",
      "Computing test accuracy on Unlearning model\n",
      "Test accuracy on Unlearning model is  0.837\n",
      "Computing forget accuracy on Unlearning model\n",
      "Forget accuracy on Unlearning model is  0.861\n",
      "Running epoch : 2\n",
      "I am now unlearning all the wrong things you taught me!!!\n",
      "Computing retain accuracy on Unlearning Model\n",
      "Retain accuracy on Unlearning Model is  0.9981555555555556\n",
      "Computing test accuracy on Unlearning model\n",
      "Test accuracy on Unlearning model is  0.8374\n",
      "Computing forget accuracy on Unlearning model\n",
      "Forget accuracy on Unlearning model is  0.848\n",
      "Running epoch : 3\n",
      "I am now unlearning all the wrong things you taught me!!!\n",
      "Computing retain accuracy on Unlearning Model\n",
      "Retain accuracy on Unlearning Model is  0.9995111111111111\n",
      "Computing test accuracy on Unlearning model\n",
      "Test accuracy on Unlearning model is  0.829\n",
      "Computing forget accuracy on Unlearning model\n",
      "Forget accuracy on Unlearning model is  0.8396\n",
      "Running epoch : 4\n",
      "I am now unlearning all the wrong things you taught me!!!\n",
      "Computing retain accuracy on Unlearning Model\n",
      "Retain accuracy on Unlearning Model is  0.9997333333333334\n",
      "Computing test accuracy on Unlearning model\n",
      "Test accuracy on Unlearning model is  0.822\n",
      "Computing forget accuracy on Unlearning model\n",
      "Forget accuracy on Unlearning model is  0.837\n",
      "Running epoch : 5\n",
      "I am now unlearning all the wrong things you taught me!!!\n",
      "Computing retain accuracy on Unlearning Model\n",
      "Retain accuracy on Unlearning Model is  0.9997111111111111\n",
      "Computing test accuracy on Unlearning model\n",
      "Test accuracy on Unlearning model is  0.819\n",
      "Computing forget accuracy on Unlearning model\n",
      "Forget accuracy on Unlearning model is  0.8358\n",
      "Running epoch : 6\n",
      "I am now unlearning all the wrong things you taught me!!!\n",
      "Computing retain accuracy on Unlearning Model\n",
      "Retain accuracy on Unlearning Model is  0.9998666666666667\n",
      "Computing test accuracy on Unlearning model\n",
      "Test accuracy on Unlearning model is  0.8162\n",
      "Computing forget accuracy on Unlearning model\n",
      "Forget accuracy on Unlearning model is  0.8332\n",
      "Running epoch : 7\n",
      "I am now unlearning all the wrong things you taught me!!!\n",
      "Computing retain accuracy on Unlearning Model\n",
      "Retain accuracy on Unlearning Model is  0.9999111111111111\n",
      "Computing test accuracy on Unlearning model\n",
      "Test accuracy on Unlearning model is  0.816\n",
      "Computing forget accuracy on Unlearning model\n",
      "Forget accuracy on Unlearning model is  0.8348\n",
      "Running epoch : 8\n",
      "I am now unlearning all the wrong things you taught me!!!\n",
      "Computing retain accuracy on Unlearning Model\n",
      "Retain accuracy on Unlearning Model is  0.9999555555555556\n",
      "Computing test accuracy on Unlearning model\n",
      "Test accuracy on Unlearning model is  0.8164\n",
      "Computing forget accuracy on Unlearning model\n",
      "Forget accuracy on Unlearning model is  0.8352\n",
      "Running epoch : 9\n",
      "I am now unlearning all the wrong things you taught me!!!\n",
      "Computing retain accuracy on Unlearning Model\n",
      "Retain accuracy on Unlearning Model is  0.9998666666666667\n",
      "Computing test accuracy on Unlearning model\n",
      "Test accuracy on Unlearning model is  0.8182\n",
      "Computing forget accuracy on Unlearning model\n",
      "Forget accuracy on Unlearning model is  0.8268\n",
      "Running epoch : 10\n",
      "I am now unlearning all the wrong things you taught me!!!\n",
      "Computing retain accuracy on Unlearning Model\n",
      "Retain accuracy on Unlearning Model is  0.9999555555555556\n",
      "Computing test accuracy on Unlearning model\n",
      "Test accuracy on Unlearning model is  0.8176\n",
      "Computing forget accuracy on Unlearning model\n",
      "Forget accuracy on Unlearning model is  0.8276\n",
      "Printing outputs_U shape  (10, 5000)\n",
      "Printing outputs_R shape  (10, 5000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34db0c463a114fa88b1de856ad404b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ret = score_unlearning_algorithm(data_loaders, pretrained_models,unlearning,10,0.01,cross_entropy_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:53:26.143398Z",
     "iopub.status.busy": "2023-12-05T02:53:26.143086Z",
     "iopub.status.idle": "2023-12-05T02:53:26.153954Z",
     "shell.execute_reply": "2023-12-05T02:53:26.153035Z",
     "shell.execute_reply.started": "2023-12-05T02:53:26.143368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_score': 0.6124313306419463,\n",
       " 'F': 0.65248486328125,\n",
       " 'unlearn_retain_accuracy': 0.9985622222222222,\n",
       " 'unlearn_test_accuracy': 0.82288,\n",
       " 'unlearn_forget_accuracy': 0.8379000000000001,\n",
       " 'retrain_retain_accuracy': 0.9952666666666666,\n",
       " 'retrain_test_accuracy': 0.8796,\n",
       " 'retrain_forget_accuracy': 0.882,\n",
       " 'retrain_outputs': array([[1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n",
       "         4.6083200e-01, 2.1786564e-03],\n",
       "        [1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n",
       "         4.6083200e-01, 2.1786564e-03],\n",
       "        [1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n",
       "         4.6083200e-01, 2.1786564e-03],\n",
       "        ...,\n",
       "        [1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n",
       "         4.6083200e-01, 2.1786564e-03],\n",
       "        [1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n",
       "         4.6083200e-01, 2.1786564e-03],\n",
       "        [1.1427237e+00, 6.8271221e-03, 5.4480130e-05, ..., 1.4155446e-01,\n",
       "         4.6083200e-01, 2.1786564e-03]], dtype=float32),\n",
       " 'unlearn_outputs': array([[1.6370027e+00, 8.7101199e-03, 4.8612305e-03, ..., 4.3062749e+00,\n",
       "         1.8149592e-02, 1.1651661e-02],\n",
       "        [4.4297514e+00, 4.0217936e-03, 2.8739529e-04, ..., 4.9859891e+00,\n",
       "         4.4260677e-03, 3.1121430e-01],\n",
       "        [4.5161128e+00, 3.3387186e-03, 6.2813214e-04, ..., 6.3128376e+00,\n",
       "         2.0238109e+00, 3.5831239e-02],\n",
       "        ...,\n",
       "        [4.5417757e+00, 2.0763958e-03, 1.5128803e-04, ..., 5.8108330e+00,\n",
       "         6.0173683e-02, 1.2311132e-01],\n",
       "        [1.7646540e+00, 7.5566657e-03, 3.0736878e-04, ..., 5.6649246e+00,\n",
       "         6.1655180e-03, 5.6460615e-02],\n",
       "        [6.2406397e+00, 1.8735495e-04, 1.6046858e-04, ..., 4.1507249e+00,\n",
       "         5.2849591e-01, 1.4684732e-01]], dtype=float32),\n",
       " 'unlearning_model': ResNet(\n",
       "   (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "   (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (relu): ReLU(inplace=True)\n",
       "   (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "   (layer1): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace=True)\n",
       "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace=True)\n",
       "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (layer2): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace=True)\n",
       "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (downsample): Sequential(\n",
       "         (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "         (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace=True)\n",
       "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (layer3): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace=True)\n",
       "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (downsample): Sequential(\n",
       "         (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "         (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace=True)\n",
       "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (layer4): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace=True)\n",
       "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (downsample): Sequential(\n",
       "         (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "         (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (relu): ReLU(inplace=True)\n",
       "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "   (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       " )}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
